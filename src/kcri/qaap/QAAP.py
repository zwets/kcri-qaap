#!/usr/bin/env python3
#
# QAAP.py - main for the KCRI Assembly and Quality Analysis Pipeline
#

import sys, os, argparse, json
from pico.workflow.logic import Workflow
from pico.workflow.executor import Executor
from pico.jobcontrol.subproc import SubprocessScheduler
from .data import QAAPBlackboard, Platform
from .services import SERVICES
from .workflow import DEPENDENCIES
from .workflow import SystemTargets, UserTargets, Services, Params
from .filescan import detect_filetype, scan_inputs, find_inputs, is_illumina_output_dir
from . import __version__

# Global variables and defaults
SERVICE, VERSION = "KCRI QAAP", __version__

# Exit with error message and non-zero code
def err_exit(msg, *args):
    print(('QAAP: %s' % msg) % args, file=sys.stderr)
    sys.exit(1)

# Parse string to UserTarget or Service, else raise error
def UserTargetOrService(s):
    try: return UserTargets(s)
    except: return Services(s)


# MAIN -------------------------------------------------------------------

def main():
    '''QAAP main program.'''

    parser = argparse.ArgumentParser(
        description="""\
The KCRI Quality Analysis and Assembly Pipeline (QAAP) performs quality
analysis and assembly of sequencer reads.  It executes a workflow of tool
invocations depending on the targets it is given.

The actions to be performed are specified using the -t/--targets option.
Each target corresponds to a recipe for a series of service invocations.
When omitted, the DEFAULT target is assumed.  The DEFAULT target performs
quality analysis and no assembly.

Use -l/--list-targets to see available TARGETS.  Use -s/--list-services
to see available SERVICES.  Combine -t DEFAULT with -x/--exclude to stop
specific targets or services from being executed.

INPUTS to the QAAP can currently be either:
- A MiSeq or NextSeq run output directory.  QAAP will process all reads
  in that run, as well as the 'InterOp' data produced by the sequencer.
- An arbitrary directory with fastq files, or a list of fastq files.
  The QAAP will attempt to determine the sequencing platform.  If it
  fails, specify the platform with option -p/--platform.
- A directory with FASTA files or a list of FASTA files.  The QAAP will
  perform QA on these.  You may want to use option -r/--reference to
  specify a reference genome.

QAAP automatically pairs up PAIRED-END reads, provided file names differ
in exactly one position, where one file has a '1' and the other a '2',
following an R, r, -, _, ., or @.

OUTPUT is generated in directories under OUT_DIR.  Sample names are
generated by removing extensions and from file names.
""",
        epilog="""\
Instead of passing arguments on the command-line, you can put them, one
per line, in a text file and pass this file with @FILENAME.
""",
        fromfile_prefix_chars='@',
        formatter_class=argparse.RawDescriptionHelpFormatter)

    # General arguments
    group = parser.add_argument_group('General parameters')
    group.add_argument('-t', '--targets',       metavar='TARGET[,...]', default='DEFAULT', help="analyses to perform [DEFAULT]")
    group.add_argument('-x', '--exclude',       metavar='TARGET_OR_SERVICE[,...]', help="targets and/or services to exclude from running")
    group.add_argument('-o', '--out-dir',       metavar='PATH', default='.', help="base directory to write output to [PWD] (must be relative when dockerised)")
    group.add_argument('-p', '--platform',      metavar='NAME', default=None, help="sequencing platform (%s), default is autodetect" % ','.join(v.value for v in Platform))
    group.add_argument('-m', '--metagenomic',   action='store_true', help="specifies that the run is metagenomic")
    group.add_argument('-a', '--amplicon',      action='store_true', help="specifies that the run is amplicon-based")
    group.add_argument('-l', '--list-targets',  action='store_true', help="list the available targets")
    group.add_argument('-s', '--list-services', action='store_true', help="list the available services")
    group.add_argument('-r', '--reference',     metavar='FASTA', help="path to a reference genome to use in QC")
    group.add_argument('-d', '--db-root',       metavar='PATH', default='/databases', help="base path to databases (leave default when dockerised)")
#    group.add_argument('-z', '--unzip',         action='store_true', help="unzip all gzipped files prior to running the pipeline (faster at the cost of disk space)")
    group.add_argument('-v', '--verbose',       action='store_true', help="write verbose output to stderr")
    group.add_argument('inputs', metavar='DIR_OR_FILES', nargs='*', default=[], help="input directory or list of fastq files")

    # Resource management arguments
    group = parser.add_argument_group('Scheduler parameters')
    group.add_argument('--max-cpus', metavar='N',   type=int, default=None, help="number of CPUs to allocate (default: all but 2)")
    group.add_argument('--max-mem',  metavar='GB',  type=int, default=None, help="total memory to allocate (default: 90%%)")
    group.add_argument('--max-disc', metavar='GB',  type=int, default=None, help="total disc space to allocate (default: 80%%)")
    group.add_argument('--max-time', metavar='SEC', type=int, default=None, help="maximum overall run time (default: unlimited)")
    group.add_argument('--poll',     metavar='SEC', type=int, default=2, help="seconds between backend polls [2]")

    # Service specific arguments
    group = parser.add_argument_group('Trimming parameters')
    group.add_argument('--tr-q', type=int, metavar='Q', default=None, help="cut-off Q score (default: 10 regular, 20 metagenomic)")
    group.add_argument('--tr-l', type=int, metavar='LEN', default=None, help="minimum read length to keep (default: 36 regular, 72 metagenomic)")
    group.add_argument('--tr-o', type=int, metavar='BASES', default=None, help="minimum adapter overlap to trim (default: 5 regular, 6 metagenomic)")
    group.add_argument('--tr-a', metavar='NAME', default=None, help="base name of the Trimmomatic adapter file [default]")

    group = parser.add_argument_group('Quast parameters')
    group.add_argument('--qu-t', type=int, metavar='LEN', default=500, help="threshold contig length for Quast (500)")

    group = parser.add_argument_group('Unicycler parameters')
    group.add_argument('--un-m', metavar='MODE', default='normal', help="unicycler mode (conservative, normal, bold)")
    group.add_argument('--un-p', metavar='SIZE', default=10000, help="polish contigs of SIZE and longer (default 10000)")

    # Perform the parsing
    args = parser.parse_args()

    # Parse targets and translate to workflow arguments
    targets = []
    try:
        targets = list(map(lambda t: UserTargets(t.strip()), args.targets.split(',') if args.targets else []))
    except ValueError as ve:
        err_exit('invalid target: %s (try --list-targets)', ve)

    # Parse excludes and translate to workflow arguments
    excludes = []
    try:
        excludes = list(map(lambda t_or_s: UserTargetOrService(t_or_s.strip()), args.exclude.split(',') if args.exclude else []))
    except ValueError as ve:
        err_exit('invalid exclude: %s (try --list-targets and --list-services)', ve)

    # Parse the platform option
    platform = None
    try:
        platform = Platform(args.platform) if args.platform else None
    except:
        err_exit('invalid platform: %s, choose from: %s' % (args.platform, ', '.join(v.value for v in Platform)))

    # Parse and validate inputs into fastqs and fastas
    illumina_run_dir = None
    il_fqs = dict()     # illumina pe reads
    pe_fqs = dict()     # other pe reads
    se_fqs = dict()     # single reads
    fastas = dict()     # fasta files
    if len(args.inputs) == 1 and os.path.isdir(args.inputs[0]):
        inp_dir = args.inputs[0]
        if os.path.isdir(os.path.join(inp_dir, 'Data','Intensities','BaseCalls')):
            if is_illumina_output_dir(inp_dir):
                illumina_run_dir = os.path.abspath(inp_dir)
            inp_dir = os.path.join(inp_dir, 'Data','Intensities','BaseCalls')
            il_fqs, pe_fqs, se_fqs, _ = find_inputs(inp_dir)
            if se_fqs: err_exit('cannot handle unpaired reads in Illumina run dir: %s' % inp_dir)
            elif pe_fqs: err_exit('QAAP autodetect broken: reads not identified as Illumina in: %s' % inp_dir)
            elif not il_fqs: err_exit('no reads found in Illumina run dir: %s' % inp_dir)
        else:
            il_fqs, pe_fqs, se_fqs, fastas = find_inputs(inp_dir)
    elif args.inputs:
        il_fqs, pe_fqs, se_fqs, fastas = scan_inputs(args.inputs, strict=True)

    # Now if illumina_run_dir is set, only il_fqs is set, else could have all sort of things

    # Perform platform consistency checks to avoid complexity explosion
    if not platform and (pe_fqs or se_fqs):
        err_exit('could not auto-detect sequencing platform, please specify -p/--platform')
    elif not platform and il_fqs:
        # @TODO@ add autodetect to tell apart MiSeq and NextSeq (relevant for TrimGalore)
        err_exit('please specify \'--platform MiSeq\' or \'--platform NextSeq\' (autodetect not yet implemented in QAAP)')
    elif platform in ( Platform.MISEQ, Platform.NEXTSEQ ) and se_fqs:
        err_exit('QAAP can\'t handle unpaired %s reads, please specify \'-p/--platform ignore\' or omit the unpaired fastqs' % platform.value)

    # Now if we have any fastqs, then we have platform, and if platform is Illumina then just PE reads

    # If Illumina, move all pe reads to il_fqs, else move any il_fqs to pe_fqs
    if platform in ( Platform.MISEQ, Platform.NEXTSEQ ): il_fqs.update(pe_fqs); pe_fqs.clear()
    else: pe_fqs.update(il_fqs); il_fqs.clear()

    # Now we have only il_fqs and platform is known Illumina, or else pe_fqs or se_fqs imply platform is ignore

    # Parse the reference parameter
    reference = None
    if args.reference:
        if not os.path.isfile(args.reference):
            err_exit('no such file: %s', args.reference)
        if detect_filetype(args.reference) != 'fasta':
            err_exit('reference not a FASTA file: %s', args.reference)
        reference = os.path.abspath(args.reference)

    # Parse the --list options
    if args.list_targets:
        print('targets:', ','.join(t.value for t in UserTargets))
    if args.list_services:
        print('services:', ','.join(s.value for s in Services))

    # Exit when no inputs were provided
    if not args.inputs:
        if not args.list_targets and not args.list_services:
            err_exit('no inputs were provided')
        else:
            sys.exit(0)

    # Check existence of the db_root directory
    if not os.path.isdir(args.db_root):
        err_exit('no such directory for --db-root: %s', args.db_root)
    db_root = os.path.abspath(args.db_root)

    # Create the output directory
    try:
        os.makedirs(args.out_dir, exist_ok=True)
    except Exception as e:
        err_exit('error creating directory %s: %s', arg.out_dir, str(e))

    # Set up the Workflow execution
    blackboard = QAAPBlackboard(args.verbose)
    blackboard.start_run(SERVICE, VERSION, vars(args))
    blackboard.put_base_path(os.path.abspath(args.out_dir))
    blackboard.put_illumina_run_dir(illumina_run_dir)
    blackboard.put_db_root(db_root)

    # Set the workflow params based on user inputs present
    params = list()
    if args.metagenomic:
        params.append(Params.META)
    if illumina_run_dir:
        params.append(Params.ILLUM_RUN)
    if il_fqs:
        params.append(Params.ILLUM_READS)
        params.append(Params.READS)
        blackboard.put_input_il_fqs(il_fqs)
    if pe_fqs:
        params.append(Params.READS)
        blackboard.put_input_pe_fqs(pe_fqs)
    if se_fqs:
        params.append(Params.READS)
        blackboard.put_input_se_fqs(se_fqs)
    if fastas:
        params.append(Params.FASTAS)
        blackboard.put_input_fastas(fastas)

    # Now that path handling has been done we can safely change our PWD
    os.chdir(args.out_dir)

    # Set up the workflow executor and the batches
    scheduler = SubprocessScheduler(args.max_cpus, args.max_mem, args.max_disc, args.max_time, args.poll, not args.verbose)
    executor = Executor(SERVICES, scheduler)
    workflow = Workflow(DEPENDENCIES, params, targets, excludes)
    executor.execute(workflow, blackboard)
    workflow = Workflow(DEPENDENCIES, params, [SystemTargets.MULTIQC], excludes)
    executor.execute(workflow, blackboard)

    # DONE, mark the end of the run end on the blackboard
    blackboard.end_run()

    # Dump the blackboard to the JSON results file
    with open('qaap-results.json', 'w') as f_json:
        json.dump(blackboard.as_dict(args.verbose), f_json)

    # Write a TSV summary?
    with open('qaap-summary.tsv', 'w') as f_tsv:
        commasep = lambda l: ','.join(l) if l else ''
        b = blackboard
        d = dict({
            's_id': 'TODO',
            'n_reads': b.get('services/ReadsMetrics/results/n_reads', 'NA'),
            'nt_read': b.get('services/ReadsMetrics/results/n_bases', 'NA'),
            'pct_q30': b.get('services/ReadsMetrics/results/pct_q30', 'NA'),
            'n_ctgs': b.get('services/ContigsMetrics/results/n_seqs', 'NA'),
            'nt_ctgs': b.get('services/ContigsMetrics/results/tot_len', 'NA'),
            'n1':  b.get('services/ContigsMetrics/results/max_len', 'NA'),
            'n50':  b.get('services/ContigsMetrics/results/n50', 'NA'),
            'l50':  b.get('services/ContigsMetrics/results/l50', 'NA'),
            'pct_gc':  b.get('services/ContigsMetrics/results/pct_gc', b.get('services/ReadsMetrics/results/pct_gc', 'NA')),
            })
        print('\t'.join(d.keys()), file=f_tsv)
        print('\t'.join(map(lambda v: v if v else '', d.values())), file=f_tsv)

    # Done done
    return 0


if __name__ == '__main__':
   main()

